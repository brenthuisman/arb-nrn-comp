Compares all DBBS models in Arbor and NEURON

# Installation

Clone this repository as `arb-nrn-comp` in your `$HOME` folder:

```
cd $HOME
git clone git@github.com:Helveg/arb-nrn-comp
```

## PizDaint specifics

For easy deployment on PizDaint, set the following env vars (in `.bashrc` for example):

```
export CPU_ACCOUNT="my-account-with-access-to-mc"
export GPU_ACCOUNT="my-account-with-access-to-gpu"
```

# Local plotting replication

Following these steps allows you to plot the plots as seen in https://www.biorxiv.org/content/10.1101/2022.03.02.482285v1.full

The plots use intermediate data, stored in pickles (a binary Python data format). The pickles reduce the dependencies required to run the plots,
and drastically reduce the time spent to plot them. If you want to replicate the findings from start to finish, you'll have to follow the [Large scale simulation replication](#large-scale-simulation-replication) instructions, delete the pickles in your local repo, and replace the files in the `results` folder with your own results. The single compartment and single cell pickles can be generated by running the scripts in the `remote` folder, in the full simulation env,
and transferring the pickles to your plotting env.

1. Set up a plotting environment

```
cd $HOME
python -m venv arbnrn-plotenv
source arbnrn-plotenv/bin/activate
pip install -r requirements.txt
```

2. You can now plot or write the plots to file using the `plot` and `build` wrappers, respectively, from the repo root:

```
# Plot some plots
python plot <name1-of-file-in-plots-folder-without-extension> <name2> <name3>
# Plot all the plots
python plot
# Write some plots to a certain format
python build <format:html,jpg,svg,png,eps> <name1> <name2>
# Write all the plots to PNG
python build png

# Examples
python plot spikes single_cell
python build html
```

# Large scale simulation replication

A no warranty list of steps to reproduce the findings in https://www.biorxiv.org/content/10.1101/2022.03.02.482285v1.full

These steps work on PizDaint, which uses [`slurm`](https://slurm.schedmd.com/documentation.html) for job management and [`modules`](http://modules.sourceforge.net/) for environment management.

## Setup Arbor

1. Create environment for the Arbor toolchain

```
cd $HOME
python -m venv arbenv
```

2. Activate the Arbor venv:

```
source $HOME/arbenv/bin/activate
```

3. Clone the arbor environment toolchain

```
cd arbenv
git clone git@github.com:Helveg/arbor --recurse-submodules
cd arbor && git checkout aba80a93b169bee93aa693c0d612bd7f66b7e5dc && cd ..
git clone git@github.com:dbbs-lab/arborize
cd arborize && git checkout 2ab8e26050782c5ce218114e50ceb550ad94b751 && cd ..
git clone git@github.com:Helveg/bsb-1 bsb
cd bsb && git checkout 1cf5a247927b52a54f870f3f2680cd9f947a8d3f && cd ..
git clone git@github.com:dbbs-lab/catalogue
cd catalogue && git checkout eb2eaee1d0563dfdca0692514ae2e25650578156 && cd ..
git clone git@github.com:dbbs-lab/glia
cd glia && git checkout 44adc91575f15a98813cd69a43a7e34d0a63b570 && cd ..
git clone git@github.com:dbbs-lab/models
cd models && git checkout dfa87f6438d2a9ac9d30a263df628060ac0bf70c && cd ..
```

3. Install the Python tools (in this order!)

```
CC=cc CXX=CC pip install mpi4py
pip install -e bsb
pip install -e models
pip install -e catalogue
pip install -e arborize
pip install -e glia
```

4. Build `arbor`:

```
mkdir arbor/build
cd arbor/build
module load daint-gpu cudatoolkit CMake gcc/9.3.0
module switch PrgEnv-cray PrgEnv-gnu
CC=cc CXX=CC cmake .. \
  -DARB_WITH_MPI=ON \
  -DARB_WITH_PROFILING=OFF \
  -DARB_GPU=cuda \
  -DARB_USE_BUNDLED_LIBS=ON \
  -DCMAKE_INSTALL_PREFIX=$HOME/arbenv \
  -DARB_WITH_PYTHON=ON \
  -DARB_VECTORIZE=ON
make install -j 8
cd $HOME
```

You can now replicate Arbor instructions.

## Setup NEURON

1. Create environment for the NEURON toolchain

```
cd $HOME
python -m venv nrnenv
```

2. Activate NEURON environment

```
source $HOME/nrnenv/bin/activate
```

3. Clone NEURON toolchain

```
cd $HOME/nrnenv
git clone git@github.com:dbbs-lab/arborize
cd arborize && git checkout 0228676a16792421b0212de647526877d2fda451 && cd ..
git clone git@github.com:dbbs-lab/bsb
cd bsb && git checkout cc03ab7a57024980003de87427388a035832005c && cd ..
git clone git@github.com:dbbs-lab/dbbs-mod-collection
cd dbbs-mod-collection && git checkout c2fa5783d3d77786c23ec03978d15ad7e461398d && cd ..
git clone git@github.com:dbbs-lab/glia
cd glia && git checkout b93f0488ca1ab4df5f9464b6bf781b7ec4a31beb && cd ..
git clone git@github.com:dbbs-lab/models
cd models && git checkout d7255f2c837808a374d57811f1ec88ff08801fef && cd ..
git clone git@github.com:Helveg/patch
cd patch && git checkout 6b7bcf8f0bb0623ec00cce0f43e59733ab895bdc && cd ..
```

4. Install the tools (in this order!)

```
CC=cc CXX=CC pip install mpi4py NEURON==8.1
pip install -e bsb
pip install -e models
pip install -e dbbs-mod-collection
pip install -e arborize
pip install -e patch
```

## Setup CoreNEURON

1. Create environment for the CoreNEURON toolchain

```
cd $HOME
python -m venv corenrnenv
```

2. Activate CoreNEURON environment

```
source $HOME/corenrnenv/bin/activate
```

3. Clone CoreNEURON toolchain

```
cd $HOME/corenrnenv
git clone git@github.com:dbbs-lab/arborize
cd arborize && git checkout 9ad43718fd9e551ea451b424ed9423e306a9ae54 && cd ..
git clone git@github.com:dbbs-lab/bsb
cd bsb && git checkout f702978eb38aece5b9d6e0ad54e133a977247541 && cd ..
git clone git@github.com:dbbs-lab/dbbs-mod-collection
cd dbbs-mod-collection && git checkout bdd7a50d9c067b143f9a412c6ed03655b3de0898 && cd ..
git clone git@github.com:dbbs-lab/glia
cd glia && git checkout ae825e090046c479dfd515ce26aaa6be3d7d7a00 && cd ..
git clone git@github.com:dbbs-lab/models
cd models && git checkout fc8b00ceb2d6133817497ec52fbb92783dbd822c && cd ..
git clone git@github.com:Helveg/patch
cd patch && git checkout 6b7bcf8f0bb0623ec00cce0f43e59733ab895bdc && cd ..
```

4. Install the tools (in this order!)

```
CC=cc CXX=CC pip install mpi4py NEURON==8.1
pip install -e bsb
pip install -e models
pip install -e dbbs-mod-collection
pip install -e arborize
pip install -e glia
pip install -e patch
```


## Deployment

Before first use, run the deployment scripts to create all of the required network model files, configuration files and jobscripts.

```
cd $HOME/arb-nrn-comp
python deploy/unpack.py
```

## Benchmarks

After deployment, any of the jobscripts can be queued manually by calling them directly with `sbatch` (always run jobs from the `$SCRATCH` filesystem):

```
cd $SCRATCH
sbatch $HOME/arb-nrn-benchmarks-rdsea-2022/jobs/nrn_distr.sh
```

This requires you to write down the job id, and parse the logs manually for the time results, and append them to the job results CSV, `jobs.csv`.

It's better to automate this process by using the `queue.py` script, which will write down the job ids and names in `jobids.csv`. You can use it
for example like this:

```
cd $SCRATCH
# Queue all benchmarks, 10 times (default)
python $HOME/arb-nrn-comp/deploy/queue.py
# Queue the `nrn_distr` and `arb_distr`, 50 times
python $HOME/arb-nrn-comp/deploy/queue.py nrn_distr arb_distr x50
```

Then, after the jobs have completed, you can use `deploy/parse.py` to parse the logs:

```
python $HOME/arb-nrn-comp/deploy/parse.py <batch_name> jobids.csv <log_dir>
```

This will create a subfolder (named `batch_name`) in the `results` folder of the git repo,
with the logs and a `jobs.csv` file with the results. You can then move `jobs.csv` to the root
of the repo to overwrite the results. The logs need to be located in `<log_dir>` (usually `$SCRATCH`).

### Reproduction

To reproduce the benchmarks follow these steps:

1. Start the benchmarks

```
cd $SCRATCH
python $HOME/arb-nrn-comp/deploy/queue.py
```

2. Wait for the benchmarks to finish, parse the logs:

```
cd $SCRATCH
python $HOME/arb-nrn-comp/deploy/parse.py job_results jobids.csv .
```

3. Substitute the given `jobs.csv` with your own results:

```
cd $HOME/arb-nrn-comp
cp results/job_results/jobs.csv jobs.csv
```

You can now plot all of the benchmark related figures with your own results.

# Single comp and cell replication

The single comp and cell results can be replicated in the same environment as the large scale simulations:

```
cd $HOME
source arbenv/bin/activate
pip install -e nrnenv/dbbs-mod-collection
pip install -e nrnenv/patch
pip install neuron==8.1
cd arb-nrn-comp
python remote/single_cell_val_arb.py
python remote/single_comp_sim.py
source ../nrnenv/bin/activate
python remote/single_cell_val_nrn.py
```

You should now have 3 new versions of the `arb_sc.pkl`, `single_comp.pkl` and `nrn_sc.pkl` files. 
You can re-plot any of the figures to view them, or transfer them from your sim env to your local
plotting env.
